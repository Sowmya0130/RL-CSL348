{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b86808d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward table:\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "Initial Q-Table:\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Defining the states and actions\n",
    "states = [0,1, 2, 3, 4, 5]\n",
    "actions = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Defining the rewards\n",
    "rewards = np.array([[-1, -1, -1, -1, 0, -1],\n",
    "                [-1, -1, -1, 0, -1, 100],\n",
    "                [-1, -1, -1, 0, -1, -1],\n",
    "                [-1, 0, 0, -1, 0, -1],\n",
    "                [0, -1, -1, 0, -1, 100],\n",
    "                [-1, 0, -1, -1, 0, 100]])\n",
    "\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((len(states), len(actions)),dtype=int)\n",
    "\n",
    "print(\"Reward table:\")\n",
    "print(rewards)\n",
    "print()\n",
    "print(\"Initial Q-Table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200f2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a36ae123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table after episode 1:\n",
      "[[ 5 75  0  0  0  0]\n",
      " [ 7  0  0  0  1 97]\n",
      " [23  0  0  3  0  0]\n",
      " [ 5 48  0  0  0  0]\n",
      " [25  0  0  0  0  0]\n",
      " [16  0  0  0  0  0]]\n",
      "Q-Table after episode 2:\n",
      "[[ 5 76  0  0  0  0]\n",
      " [ 7  0  0  0  1 98]\n",
      " [27  0  0  3  0  0]\n",
      " [ 5 48  0  0  0  0]\n",
      " [25  0  0  0  0  0]\n",
      " [16  0  0  0  0  0]]\n",
      "Q-Table after episode 3:\n",
      "[[ 5 76  0  0  0  0]\n",
      " [ 7  0  0  0  1 98]\n",
      " [27  0  0  3  0  0]\n",
      " [ 5 48  0  0  0  0]\n",
      " [25  0  0  0  0  0]\n",
      " [16  0  0  0  0  0]]\n",
      "Q-Table after episode 4:\n",
      "[[ 5 76  0  0  0  0]\n",
      " [ 7  0  0  0  1 98]\n",
      " [27  0  0  3  0  0]\n",
      " [ 5 48  0  0  0  0]\n",
      " [25  0  0  0  0  0]\n",
      " [16  0  0  0  0  0]]\n",
      "Q-Table after episode 5:\n",
      "[[ 5 77  0  0  0  0]\n",
      " [ 7  0  0  0  1 99]\n",
      " [27  0  0  3  0  0]\n",
      " [ 5 48  0  0  0  0]\n",
      " [29  0  0  0  0  0]\n",
      " [16  0  0  0  0  0]]\n",
      "Q-Table after episode 6:\n",
      "[[  5  78   0   0   0   0]\n",
      " [  7   0   0   0   1 100]\n",
      " [ 27   0   0   3   0   0]\n",
      " [  5  48   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 7:\n",
      "[[  5  78   0   0   0   0]\n",
      " [  7   0   0   0   1 101]\n",
      " [ 27   0   0   3   0   0]\n",
      " [  5  48   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 8:\n",
      "[[  5  79   0   0   0   0]\n",
      " [  7   0   0   0   1 102]\n",
      " [ 31   0   0   3   0   0]\n",
      " [  5  48   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 9:\n",
      "[[  5  79   0   0   0   0]\n",
      " [  7   0   0   0   1 103]\n",
      " [ 31   0   0   3   0   0]\n",
      " [  5  48   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 10:\n",
      "[[  5  80   0   0   0   0]\n",
      " [  7   0   0   0   1 104]\n",
      " [ 34   0   0   3   0   0]\n",
      " [  5  48   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 11:\n",
      "[[  5  80   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 34   0   0   3   0   0]\n",
      " [  5  52   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 12:\n",
      "[[  5  81   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  52   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 13:\n",
      "[[  5  81   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 14:\n",
      "[[  5  81   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 15:\n",
      "[[  5  81   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 16:\n",
      "[[  5  81   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 17:\n",
      "[[  5  82   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 33   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 18:\n",
      "[[  5  83   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 19:\n",
      "[[  5  83   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 20:\n",
      "[[  5  83   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 37   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 21:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 22:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 23:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  56   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 24:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  59   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 25:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  59   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 26:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  59   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 27:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  59   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 28:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  59   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 29:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  59   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 30:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  62   0   0   0   0]\n",
      " [ 37   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 31:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  62   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 32:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  65   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 33:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  65   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 34:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  65   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 35:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 40   0   0   3   0   0]\n",
      " [  5  67   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 36:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 43   0   0   3   0   0]\n",
      " [  5  67   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 37:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 46   0   0   3   0   0]\n",
      " [  5  67   0   0   0   0]\n",
      " [ 40   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 38:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 46   0   0   3   0   0]\n",
      " [  5  67   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 39:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 46   0   0   3   0   0]\n",
      " [  5  67   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 40:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 46   0   0   3   0   0]\n",
      " [  5  67   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 41:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 46   0   0   3   0   0]\n",
      " [  5  69   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 42:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 46   0   0   3   0   0]\n",
      " [  5  69   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 43:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 48   0   0   3   0   0]\n",
      " [  5  69   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 44:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 48   0   0   3   0   0]\n",
      " [  5  69   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 45:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 50   0   0   3   0   0]\n",
      " [  5  69   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 46:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 50   0   0   3   0   0]\n",
      " [  5  71   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 47:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 52   0   0   3   0   0]\n",
      " [  5  71   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 48:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 52   0   0   3   0   0]\n",
      " [  5  71   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 49:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 52   0   0   3   0   0]\n",
      " [  5  71   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n",
      "Q-Table after episode 50:\n",
      "[[  5  84   0   0   0   0]\n",
      " [  7   0   0   0   1 105]\n",
      " [ 52   0   0   3   0   0]\n",
      " [  5  71   0   0   0   0]\n",
      " [ 43   0   0   0   0   0]\n",
      " [ 16   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "def q_learning(state, q_table, rewards, alpha, gamma):\n",
    "    action = np.argmax(q_table[state, :])\n",
    "    next_state = action\n",
    "    reward = rewards[state, action]\n",
    "    q_table[state, action] = q_table[state, action] + int(alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action]))\n",
    "    return next_state, q_table\n",
    "\n",
    "# Run the Q-Learning algorithm for 50 episodes\n",
    "for episode in range(50):\n",
    "    state = np.random.choice(states)\n",
    "    while state != 5:\n",
    "        state, q_table = q_learning(state, q_table, rewards, alpha, gamma)\n",
    "    print(f\"Q-Table after episode {episode + 1}:\")\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the states and actions\n",
    "states = [0, 1, 2, 3, 4, 5]\n",
    "actions = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Define the rewards\n",
    "rewards = np.array([[-1, -1, -1, -1, 0, -1],\n",
    "                    [-1, -1, -1, 0, -1, 100],\n",
    "                    [-1, -1, -1, 0, -1, -1],\n",
    "                    [-1, 0, 0, -1, 0, -1],\n",
    "                    [0, -1, -1, 0, -1, 100],\n",
    "                    [-1, 0, -1, -1, 0, 100]])\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((len(states), len(actions)), dtype=int)\n",
    "\n",
    "# Set the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Run the Q-Learning algorithm for 50 episodes\n",
    "for episode in range(50):\n",
    "    # Set the initial state\n",
    "    state = np.random.choice(states)\n",
    "    while state != 5:\n",
    "        # Choose the action with the highest Q-value\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        # Update the Q-value for the chosen action\n",
    "        next_state = action\n",
    "        reward = rewards[state, action]\n",
    "        q_table[state, action] = q_table[state, action] + int(alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action]))\n",
    "        state = next_state\n",
    "    # Print the updated Q-table after each episode\n",
    "    print(f\"Q-Table after episode {episode + 1}:\")\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bfa491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m env \u001b[38;5;241m=\u001b[39m make(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Use FrozenLake-v1 instead of FrozenLake-v0\u001b[39;00m\n\u001b[0;32m     65\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearning(env)\n\u001b[1;32m---> 66\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mQLearning.learn\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     27\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Select an action\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Take the action and observe the outcome\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mQLearning.epsilon_greedy_policy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "\n",
    "    def learn(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            print(f\"Episode: {episode}\")\n",
    "\n",
    "            # Reset the environment and initialize the episode\n",
    "            state = env.reset()\n",
    "\n",
    "            # Run the episode\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Select an action\n",
    "                action = self.epsilon_greedy_policy(state)\n",
    "\n",
    "                # Take the action and observe the outcome\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Update the Q-table\n",
    "                self.Q[state, action] += self.alpha * (reward + self.gamma * np.max(self.Q[next_state]) - self.Q[state, action])\n",
    "\n",
    "                # Update the state\n",
    "                state = next_state\n",
    "           \n",
    "            # Display Q-table\n",
    "            print(\"Q-table:\")\n",
    "            print(\"---------------------------------\")\n",
    "            for row in self.Q:\n",
    "                for item in row:\n",
    "                    print(f\"{item:.3f} \", end='')\n",
    "                print()\n",
    "            print(\"---------------------------------\")\n",
    "\n",
    "            # Display R-table\n",
    "            print(\"\\nR-table:\")\n",
    "            print(\"---------------------------------\")\n",
    "            for state in env.env.P:\n",
    "                print(f\"State {state}:\")\n",
    "                for action, probabilities in env.env.P[state].items():\n",
    "                    print(f\"    Action {action}:\")\n",
    "                    for next_state, probability, reward, _ in probabilities:\n",
    "                        print(f\"        Next state: {next_state}, Probability: {probability:.3f}, Reward: {reward}\")\n",
    "            print(\"---------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from gym import make\n",
    "    env = make(\"FrozenLake-v1\")  # Use FrozenLake-v1 instead of FrozenLake-v0\n",
    "\n",
    "    agent = QLearning(env)\n",
    "    agent.learn(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
